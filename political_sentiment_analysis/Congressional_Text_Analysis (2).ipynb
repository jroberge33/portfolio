{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c21104-6840-4ecd-a73c-2ed9105fcbc1",
   "metadata": {},
   "source": [
    "# Congressional Text Analysis\n",
    "\n",
    "The code below is to analyze congressional records over the past decade, to highlight key trends and delve into political polarization by party over time. Speaker analysis is WIP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d9e1f-ee7b-4ae5-aaea-1e1eebfd1401",
   "metadata": {},
   "source": [
    "## Key Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4f25150-174e-4169-b61d-7a6bcfe4d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from urllib.parse import urlencode\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0e09c-f54a-4bc6-b1f2-1f1c5ca646f9",
   "metadata": {},
   "source": [
    "## Key Data Aggregation and Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63a9a4-3fca-4825-a6e9-dbc788e0619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scrapeops_url(api_key, url):\n",
    "    ''' function to scrape a given public url using ScrapeOps API'''\n",
    "    \n",
    "    payload = {'api_key': api_key, 'url': url}\n",
    "    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n",
    "    \n",
    "    return proxy_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f0f1c9-a373-4807-81a7-87be55589092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_congressional_record_data(starting_congress, ending_congress, api_key):\n",
    "    ''' function to aggregate congressional record urls for a certain period of congresses '''\n",
    "    \n",
    "    all_links = []\n",
    "\n",
    "    # iterate through congresses\n",
    "    for congress in range(starting_congress, ending_congress + 1):\n",
    "\n",
    "        # produce generic congressional session link associated with this congress\n",
    "        url = f\"https://www.congress.gov/congressional-record/{congress}th-congress/browse-by-date\"\n",
    "\n",
    "        # scrape data associated with that URL\n",
    "        r = requests.get(get_scrapeops_url(api_key, url))\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # create list of all congressional record pdf links within the session's site, accounting for link format change after 116th congress  \n",
    "        if congress > 116:\n",
    "            pdf_links = soup.find_all('a', href=re.compile(r'(/\\d{3}/crec/\\d{4}/\\d{2}/\\d{2}/\\d{3}/\\d{3}/CREC-\\d{4}-\\d{2}-\\d{2}\\.pdf)'))\n",
    "        else:\n",
    "            pdf_links = soup.find_all('a', href=re.compile(r'(/\\d{3}/crec/\\d{4}/\\d{2}/\\d{2}/CREC-\\d{4}-\\d{2}-\\d{2}\\.pdf)'))\n",
    "\n",
    "        all_links.extend([('https://www.congress.gov' + link['href']) for link in pdf_links])\n",
    "\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd87c944-f75d-4851-a34a-a7e828ed75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_from_url(pdf_url):\n",
    "    ''' function to read in daily congressional record pdf urls '''\n",
    "    \n",
    "    # Send a GET request to the PDF URL\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the PDF file in binary mode\n",
    "        with open(\"temp_pdf.pdf\", 'wb') as pdf_file:\n",
    "            # Write the content of the response to the PDF file\n",
    "            pdf_file.write(response.content)\n",
    "\n",
    "        # Read the text from the downloaded PDF\n",
    "        text = read_pdf_text(\"temp_pdf.pdf\")\n",
    "\n",
    "        # Delete the temporary PDF file\n",
    "        import os\n",
    "        os.remove(\"temp_pdf.pdf\")\n",
    "\n",
    "        return text\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba3000-0bf9-4c9e-8e13-4c82ec4a8da0",
   "metadata": {},
   "source": [
    "## Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f51a23f-547e-4f3b-9f13-c4934b793959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' Remove newline characters, hyphens, and punctuation marks '''\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\n|-', ' ', text)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa92c3e-c556-4d1e-96c6-7eb90733165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    ''' remove key stopwords from text '''\n",
    "    \n",
    "    # Get list of stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords from the list of words\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word != 'f']\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39978ce2-1c72-4b6a-abf4-c9569aca2d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_word(word):\n",
    "    ''' check if word exists in WordNet '''\n",
    "    return wordnet.synsets(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f295b6-8052-4108-9ed3-f387f2f82b6a",
   "metadata": {},
   "source": [
    "## Monthly Text Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3ec108-f00b-4c51-be63-e32afa02e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pdf_urls(url_list):\n",
    "    ''' iterate over list of urls and urls by month in a dct '''\n",
    "    \n",
    "    pdf_urls_by_month = {}\n",
    "    # Iterate over the list of PDF URLs\n",
    "    for pdf_url in pdf_urls:\n",
    "        # Extract the date from the URL\n",
    "        date = pdf_url.split('/')[-1]  # Extract the date from the second to last part of the URL\n",
    "        year = date.split('-')[1]\n",
    "        month = date.split('-')[2]\n",
    "        day = date.split('-')[3]\n",
    "        \n",
    "        # Construct the month key (YYYY-MM)\n",
    "        month_key = f\"{year}-{month}\"\n",
    "        if month_key in pdf_urls_by_month:\n",
    "            pdf_urls_by_month[month_key].append(pdf_url)\n",
    "        else:\n",
    "            pdf_urls_by_month[month_key] = [pdf_url]\n",
    "\n",
    "    return pdf_urls_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deec76bb-06ea-4881-a0ca-fc30eec1ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_through_pdfs(url_monthly_dct):\n",
    "    ''' iterate through and read URLs, cleaning the text and producing monthly text blobs '''\n",
    "    \n",
    "    monthly_words_dct = {}\n",
    "\n",
    "    # iterate through month, list of urls by month in dct\n",
    "    for key, value in url_monthly_dct.items():\n",
    "        print(key)\n",
    "        monthly_pdf_words = []\n",
    "\n",
    "        # iterate over each url in that mlonth\n",
    "        for pdf_url in value:\n",
    "\n",
    "            # read in text, clean and tokenize the text, remove stopwords  \n",
    "            pdf_text = read_pdf_from_url(pdf_url)     \n",
    "            cleaned_text = clean_text(pdf_text)\n",
    "            words = word_tokenize(cleaned_text)\n",
    "            filtered_words = remove_stopwords(words)\n",
    "\n",
    "            # confirm words are words in WordNet library, add words to monthly list of words\n",
    "            valid_words = [word for word in filtered_words if is_word(word)]\n",
    "            monthly_pdf_words.extend(valid_words)\n",
    "        monthly_words_dct[key] = monthly_pdf_words\n",
    "    return monthly_words_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e16ee0-7de6-4cd8-beb0-9455628ae1cc",
   "metadata": {},
   "source": [
    "## Word Counting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ce4ad8-9733-408f-95cd-45139ab256a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_words(filtered_words):\n",
    "    ''' function to return a frequency dct of words with a frequency of over 20 in a given text list '''\n",
    "    \n",
    "    # Count the occurrences of each word\n",
    "    word_counts = Counter(filtered_words)\n",
    "    \n",
    "    # Get the most common words and their frequencies\n",
    "    most_common_words = word_counts.most_common()\n",
    "    most_common_words_dct = dict(most_common_words)\n",
    "\n",
    "    common_words = {word: count for word, count in most_common_words_dct.items() if count > 20}\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b952db4-7ace-4dee-a160-382d7802b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_word_counts_df(monthly_words_dct):\n",
    "    ''' function to produce monthly frequency df from monthly words dictionary '''\n",
    "    \n",
    "    monthly_word_count_lst = []\n",
    "    months = []\n",
    "\n",
    "    # iterate through each month, list of words associated with that month \n",
    "    for key, value in monthly_words_dct.items():\n",
    "\n",
    "        # find words with frequency over 20 and their associated frequencies\n",
    "        monthly_word_counts = count_words(value)\n",
    "        monthly_word_count_lst.append(monthly_word_counts)\n",
    "        months.append(key)\n",
    "\n",
    "    # to reduce redundancies in word countings\n",
    "    all_words = set()\n",
    "    for monthly_word_count in monthly_word_count_lst:  # Assuming monthly_counts_list contains all monthly count dictionaries\n",
    "        all_words.update(monthly_word_count.keys())\n",
    "    all_words = list(all_words)\n",
    "    df = pd.DataFrame(columns=all_words)\n",
    "\n",
    "    # fill in df with monthly counts of that word\n",
    "    for i, monthly_counts in enumerate(monthly_word_count_lst):\n",
    "        month_data = {word: monthly_counts.get(word, 0) for word in all_words}\n",
    "        df.loc[i] = month_data\n",
    "    df['month'] = months\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    display(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf79956-b83e-42e5-a545-bdc694d1105c",
   "metadata": {},
   "source": [
    "## Frequency Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903a848e-545b-4680-ac9d-92f51c0a6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequencies(df, words):\n",
    "    ''' function to graph word frequencies over time with rolling avg '''\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for word in words:\n",
    "        if word in df.columns:\n",
    "            plt.plot(df['Month'], df[word], label=word)\n",
    "    \n",
    "    # If there is only one word, add a 12-month rolling average trend line\n",
    "    if len(words) == 1:\n",
    "        word = words[0]\n",
    "        if word in df.columns:\n",
    "            rolling_avg = df[word].rolling(window=12).mean()\n",
    "            plt.plot(df['Month'], rolling_avg, label=f'{word} 12-Month Rolling Avg', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Word Frequencies Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea824086-4393-42fc-b0d4-bdad087013e9",
   "metadata": {},
   "source": [
    "## Sentiment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8510fe72-4d89-4085-a3bd-d29b2dee2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(df):\n",
    "    ''' function to add sentiment column to word count df with monthly sentiment scores '''\n",
    "    \n",
    "    # Initialize an empty list to store sentiment scores for each month\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Initialize variables to accumulate sentiment and word frequencies for the current month\n",
    "        month_sentiment = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        # Iterate over each word and its frequency in the current row\n",
    "        for word, frequency in row.items():\n",
    "            # Skip the 'Month' column\n",
    "            if word != 'Month' and word != 'month':\n",
    "                try:\n",
    "                    # Convert frequency to a float\n",
    "                    frequency = float(frequency)\n",
    "                except ValueError:\n",
    "                    # Print a message if frequency is not a number and skip this word\n",
    "                    print(f\"Non-numeric frequency found: word='{word}', frequency='{frequency}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate the sentiment polarity of the word using TextBlob\n",
    "                sentiment = TextBlob(word).sentiment.polarity\n",
    "                # Accumulate the sentiment weighted by the word's frequency\n",
    "                month_sentiment += sentiment * frequency\n",
    "                # Accumulate the total word frequencies\n",
    "                total_words += frequency\n",
    "        \n",
    "        # Calculate the average sentiment for the month if there are any words\n",
    "        if total_words != 0:\n",
    "            average_sentiment = month_sentiment / total_words\n",
    "        else:\n",
    "            average_sentiment = 0\n",
    "        \n",
    "        # Append the average sentiment for the month to the list\n",
    "        sentiment_scores.append(average_sentiment)\n",
    "    \n",
    "    # Add the sentiment scores as a new column in the DataFrame\n",
    "    df['Sentiment'] = sentiment_scores\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b13db063-513a-4f9a-aade-06c0cd0452eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_over_time(df):\n",
    "    ''' function to plot sentiment over time with moving avg and r-squared values '''\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Convert 'Month' to datetime if not already\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%Y-%m')\n",
    "    df = df.sort_values('Month')\n",
    "    \n",
    "    # Plot the original sentiment scores\n",
    "    plt.plot(df['Month'], df['Sentiment'], label='Sentiment')\n",
    "    \n",
    "    # Calculate and plot the 12-month rolling average\n",
    "    df['12_Month_Rolling_Avg'] = df['Sentiment'].rolling(window=12).mean()\n",
    "    plt.plot(df['Month'], df['12_Month_Rolling_Avg'], label='12-Month Rolling Avg', linestyle='--')\n",
    "    \n",
    "    # Calculate and plot the overall trend line\n",
    "    x = list(range(len(df['Month'])))\n",
    "    y = df['Sentiment'].values\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    plt.plot(df['Month'], intercept + slope * pd.Series(x), label='Overall Trend Line', linestyle=':')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.title('Sentiment Over Time')\n",
    "    plt.legend()\n",
    "    \n",
    "    # # Print the R^2 values and the line of best fit equations\n",
    "    # print(f\"Overall line of best fit: y = {slope:.4f}x + {intercept:.4f}\")\n",
    "    # print(f\"Overall R^2: {r_value**2:.4f}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27410385-5c45-434b-be3a-4004a5a30174",
   "metadata": {},
   "source": [
    "## Driver Function and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6090aa-59fa-498a-b249-4cc7aa996369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_function(starting_congress, ending_congress, api_key, pkl_exists):\n",
    "    ''' driver function to execute data scraping and aggregation of congressional records '''\n",
    "\n",
    "    # check if pkl exists to not repeat scraping if done already '''\n",
    "    if pkl_exists:\n",
    "        with open('data.pkl', 'rb') as f:\n",
    "            monthly_words_dct = pickle.load(f)\n",
    "            return monthly_words_dct\n",
    "    else:\n",
    "        # get urls and group them by month\n",
    "        pdf_urls = get_congressional_record_data(starting_congress, ending_congress, api_key)\n",
    "        pdf_urls_by_month = group_pdf_urls(pdf_urls)\n",
    "\n",
    "        # pull monthly text associated with that month's congressional records\n",
    "        monthly_words_dct = read_through_pdfs(pdf_urls_by_month)\n",
    "\n",
    "    # produce monthly count df with words > 20 mentions in that month, sorting it \n",
    "    monthly_word_count_df = get_monthly_word_counts_df(monthly_words_dct)\n",
    "    monthly_word_count_df.sort_values(by='Month', inplace=True)\n",
    "    monthly_word_count_df['Month'] = pd.to_datetime(monthly_word_count_df['Month'])\n",
    "    return monthly_words_dct, monthly_word_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18683c10-1ea9-405b-83c8-3c88ab62e3c7",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a06693d0-0990-4f6d-9b8e-f0f4f7c0be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_congress = 113\n",
    "ending_congress = 118\n",
    "api_key = '2dc77943-2998-4800-ba89-49904eb04200'\n",
    "pkl_exists = True\n",
    "words = ['inflation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3720e-70ab-4597-a3c4-7b21bb8eac81",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8ee6a-fac0-4319-ab89-470b1076c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_words_dct, monthly_word_count_df = driver_function(starting_congress, ending_congress, api_key, pkl_exists)\n",
    "plot_word_frequencies(monthly_word_count_df, words)\n",
    "calculate_sentiment(monthly_word_count_df)\n",
    "plot_sentiment_over_time(monthly_word_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c6234-ba59-46ca-95fa-bc474f3e6210",
   "metadata": {},
   "source": [
    "## WIP Code to Analyze Speaker and Party Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412e0cf-7e6d-499d-bbde-b3ad86d59964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speakers(text):\n",
    "    # Using regular expression to extract speaker dialogues\n",
    "    speaker_pattern = re.compile(r'\\b(?:Ms\\.|Mr\\.|Mrs\\.) ([A-Z]+)\\b')\n",
    "    dialogues = []\n",
    "    \n",
    "    for match in speaker_pattern.finditer(text):\n",
    "        start = match.end()\n",
    "        speaker = match.group(1)\n",
    "        \n",
    "        end = text.find('\\n', start)\n",
    "        dialogue = text[start:end].strip()\n",
    "        \n",
    "        dialogues.append((speaker, dialogue))\n",
    "    \n",
    "    return dialogues\n",
    "\n",
    "def determine_party(speaker, rep_list, dem_list):\n",
    "    if speaker in rep_list:\n",
    "        return 'Republican'\n",
    "    elif speaker in dem_list:\n",
    "        return 'Democrat'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_party_sentiment(monthly_data, republican_congressppl, democratic_congressppl):\n",
    "    monthly_sentiments = defaultdict(lambda: {'Democrat': [], 'Republican': []})\n",
    "    \n",
    "    for month, text in monthly_data.items():\n",
    "        # Extract dialogues\n",
    "        dialogues = extract_speakers(text)\n",
    "        \n",
    "        for speaker, dialogue in dialogues:\n",
    "            # Determine speaker's party\n",
    "            party = determine_party(speaker, republican_congressppl, democratic_congressppl)\n",
    "            \n",
    "            if party:\n",
    "                # Limit dialogue to 100 words\n",
    "                limited_dialogue = ' '.join(dialogue.split()[:100])\n",
    "                \n",
    "                # Calculate sentiment\n",
    "                sentiment = TextBlob(limited_dialogue).sentiment.polarity\n",
    "                \n",
    "                # Add sentiment to the respective party list\n",
    "                monthly_sentiments[month][party].append(sentiment)\n",
    "    \n",
    "    # Calculate average sentiment for each party by month\n",
    "    average_monthly_sentiments = {\n",
    "        month: {\n",
    "            'Democrat': (sum(scores['Democrat']) / len(scores['Democrat']) if scores['Democrat'] else 0),\n",
    "            'Republican': (sum(scores['Republican']) / len(scores['Republican']) if scores['Republican'] else 0)\n",
    "        }\n",
    "        for month, scores in monthly_sentiments.items()\n",
    "    }\n",
    "    \n",
    "    return average_monthly_sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2106129-ccf2-4a46-85be-451250b2348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_over_time(average_monthly_sentiments):\n",
    "    # Prepare data for plotting\n",
    "    data = {\n",
    "        'Month': [],\n",
    "        'Democrat_Sentiment': [],\n",
    "        'Republican_Sentiment': []\n",
    "    }\n",
    "    \n",
    "    for month, sentiments in sorted(average_monthly_sentiments.items()):\n",
    "        data['Month'].append(month)\n",
    "        data['Democrat_Sentiment'].append(sentiments['Democrat'])\n",
    "        data['Republican_Sentiment'].append(sentiments['Republican'])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%Y-%m')\n",
    "    df = df.sort_values('Month')\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    plt.plot(df['Month'], df['Democrat_Sentiment'], label='Democrat Sentiment', color='blue')\n",
    "    plt.plot(df['Month'], df['Republican_Sentiment'], label='Republican Sentiment', color='red')\n",
    "    \n",
    "    # Calculate and plot the 12-month rolling average\n",
    "    df['Democrat_Rolling_Avg'] = df['Democrat_Sentiment'].rolling(window=12).mean()\n",
    "    df['Republican_Rolling_Avg'] = df['Republican_Sentiment'].rolling(window=12).mean()\n",
    "    \n",
    "    plt.plot(df['Month'], df['Democrat_Rolling_Avg'], label='Democrat 12-Month Rolling Avg', linestyle='--', color='blue')\n",
    "    plt.plot(df['Month'], df['Republican_Rolling_Avg'], label='Republican 12-Month Rolling Avg', linestyle='--', color='red')\n",
    "    \n",
    "\n",
    "    for party in ['Democrat', 'Republican']:\n",
    "        for period, label_suffix in [(pre_2017, 'Pre-2017'), (post_2017, '2017-Present')]:\n",
    "            x = list(range(len(period)))\n",
    "            y = period[f'{party}_Sentiment'].values\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            \n",
    "            plt.plot(period['Month'], intercept + slope * pd.Series(x), label=f'{party} Trend Line {label_suffix}', linestyle=':')\n",
    "            \n",
    "            # Print the R^2 values and the line of best fit equations\n",
    "            print(f\"{party} {label_suffix} line of best fit: y = {slope:.4f}x + {intercept:.4f}\")\n",
    "            print(f\"{party} {label_suffix} R^2: {r_value**2:.4f}\")\n",
    "    \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.title('Sentiment Over Time by Party')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
